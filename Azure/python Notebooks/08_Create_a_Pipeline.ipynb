{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "view-in-github",
                "colab_type": "text"
            },
            "source": [
                "<a href=\"https://colab.research.google.com/github/ayoub-kplr/AI-Architecture-Cloud/blob/main/Azure/python%20Notebooks/08_Create_a_Pipeline.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "oboERcwYuuOZ"
            },
            "source": [
                "# Cr\u00e9er un pipeline\n",
                "\n",
                "Vous pouvez effectuer les diff\u00e9rentes \u00e9tapes requises pour ing\u00e9rer des donn\u00e9es, former un mod\u00e8le et inscrire le mod\u00e8le individuellement en utilisant le SDK Azure ML pour ex\u00e9cuter des exp\u00e9riences bas\u00e9es sur des scripts. Cependant, dans un environnement d'entreprise, il est courant d'encapsuler la s\u00e9quence d'\u00e9tapes discr\u00e8tes n\u00e9cessaires pour cr\u00e9er une solution d'apprentissage automatique dans un * pipeline * qui peut \u00eatre ex\u00e9cut\u00e9 sur une ou plusieurs cibles de calcul\u00a0; soit \u00e0 la demande d'un utilisateur, \u00e0 partir d'un processus de construction automatis\u00e9, soit selon un calendrier.\n",
                "\n",
                "Dans ce notebook, vous allez rassembler tous ces \u00e9l\u00e9ments pour cr\u00e9er un pipeline simple qui pr\u00e9traite les donn\u00e9es, puis forme et enregistre un mod\u00e8le.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "OtxNFZPtuuOc"
            },
            "source": [
                "## Connectez-vous \u00e0 votre espace de travail\n",
                "\n",
                "Pour commencer, connectez-vous \u00e0 votre espace de travail.\n",
                "\n",
                "> **Remarque**\u00a0: Si vous n'avez pas encore \u00e9tabli de session authentifi\u00e9e avec votre abonnement Azure, vous serez invit\u00e9 \u00e0 vous authentifier en cliquant sur un lien, en saisissant un code d'authentification et en vous connectant \u00e0 Azure.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "gather": {
                    "logged": 1649367166880
                },
                "vscode": {
                    "languageId": "python"
                },
                "id": "k3JDQ_aluuOd"
            },
            "outputs": [],
            "source": [
                "import azureml.core\n",
                "from azureml.core import Workspace\n",
                "\n",
                "# Load the workspace from the saved config file\n",
                "ws = Workspace.from_config()\n",
                "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "XUrTk3T2uuOe"
            },
            "source": [
                "##\u00a0Pr\u00e9parer les donn\u00e9es\n",
                "\n",
                "Dans votre pipeline, vous utiliserez un ensemble de donn\u00e9es contenant des d\u00e9tails sur les patients diab\u00e9tiques. Ex\u00e9cutez la cellule ci-dessous pour cr\u00e9er ce jeu de donn\u00e9es (si vous l'avez cr\u00e9\u00e9 pr\u00e9c\u00e9demment, le code trouvera la version existante)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "gather": {
                    "logged": 1649367169674
                },
                "vscode": {
                    "languageId": "python"
                },
                "id": "4mqrs5w6uuOe"
            },
            "outputs": [],
            "source": [
                "from azureml.core import Dataset\n",
                "from azureml.data.datapath import DataPath\n",
                "\n",
                "default_ds = ws.get_default_datastore()\n",
                "\n",
                "if 'diabetes dataset' not in ws.datasets:\n",
                "    Dataset.File.upload_directory(src_dir='data',\n",
                "                              target=DataPath(default_ds, 'diabetes-data/')\n",
                "                              )\n",
                "\n",
                "    #Create a tabular dataset from the path on the datastore (this may take a short while)\n",
                "    tab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'diabetes-data/*.csv'))\n",
                "\n",
                "    # Register the tabular dataset\n",
                "    try:\n",
                "        tab_data_set = tab_data_set.register(workspace=ws, \n",
                "                                name='diabetes dataset',\n",
                "                                description='diabetes data',\n",
                "                                tags = {'format':'CSV'},\n",
                "                                create_new_version=True)\n",
                "        print('Dataset registered.')\n",
                "    except Exception as ex:\n",
                "        print(ex)\n",
                "else:\n",
                "    print('Dataset already registered.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "0mHLnPn_uuOf"
            },
            "source": [
                "## Cr\u00e9er des scripts pour les \u00e9tapes du pipeline\n",
                "\n",
                "Les pipelines consistent en une ou plusieurs *\u00e9tapes*, qui peuvent \u00eatre des scripts Python, ou des \u00e9tapes sp\u00e9cialis\u00e9es comme une \u00e9tape de transfert de donn\u00e9es qui copie les donn\u00e9es d'un emplacement \u00e0 un autre. Chaque \u00e9tape peut s'ex\u00e9cuter dans son propre contexte de calcul. Dans cet exercice, vous allez cr\u00e9er un pipeline simple contenant deux \u00e9tapes de script Python\u00a0: une pour pr\u00e9traiter certaines donn\u00e9es d'entra\u00eenement et une autre pour utiliser les donn\u00e9es pr\u00e9trait\u00e9es pour entra\u00eener et enregistrer un mod\u00e8le.\n",
                "\n",
                "Commen\u00e7ons par cr\u00e9er un dossier pour les fichiers de script que nous utiliserons dans les \u00e9tapes du pipeline.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "gather": {
                    "logged": 1649367172572
                },
                "vscode": {
                    "languageId": "python"
                },
                "id": "W4tBkfBGuuOf"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "# Create a folder for the pipeline step files\n",
                "experiment_folder = 'diabetes_pipeline'\n",
                "os.makedirs(experiment_folder, exist_ok=True)\n",
                "\n",
                "print(experiment_folder)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "TC5AzSMquuOg"
            },
            "source": [
                "Cr\u00e9ons maintenant le premier script, qui lira les donn\u00e9es de l'ensemble de donn\u00e9es sur le diab\u00e8te et appliquera un pr\u00e9traitement simple pour supprimer toutes les lignes avec des donn\u00e9es manquantes et normaliser les caract\u00e9ristiques num\u00e9riques afin qu'elles soient sur une \u00e9chelle similaire.\n",
                "\n",
                "Le script inclut un argument nomm\u00e9 **--prepped-data**, qui fait r\u00e9f\u00e9rence au dossier dans lequel les donn\u00e9es r\u00e9sultantes doivent \u00eatre enregistr\u00e9es.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "vscode": {
                    "languageId": "python"
                },
                "id": "Qm4_Jp60uuOg"
            },
            "outputs": [],
            "source": [
                "%%writefile $experiment_folder/prep_diabetes.py\n",
                "# Import libraries\n",
                "import os\n",
                "import argparse\n",
                "import pandas as pd\n",
                "from azureml.core import Run\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "\n",
                "# Get parameters\n",
                "parser = argparse.ArgumentParser()\n",
                "parser.add_argument(\"--input-data\", type=str, dest='raw_dataset_id', help='raw dataset')\n",
                "parser.add_argument('--prepped-data', type=str, dest='prepped_data', default='prepped_data', help='Folder for results')\n",
                "args = parser.parse_args()\n",
                "save_folder = args.prepped_data\n",
                "\n",
                "# Get the experiment run context\n",
                "run = Run.get_context()\n",
                "\n",
                "# load the data (passed as an input dataset)\n",
                "print(\"Loading Data...\")\n",
                "diabetes = run.input_datasets['raw_data'].to_pandas_dataframe()\n",
                "\n",
                "# Log raw row count\n",
                "row_count = (len(diabetes))\n",
                "run.log('raw_rows', row_count)\n",
                "\n",
                "# remove nulls\n",
                "diabetes = diabetes.dropna()\n",
                "\n",
                "# Normalize the numeric columns\n",
                "scaler = MinMaxScaler()\n",
                "num_cols = ['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree']\n",
                "diabetes[num_cols] = scaler.fit_transform(diabetes[num_cols])\n",
                "\n",
                "# Log processed rows\n",
                "row_count = (len(diabetes))\n",
                "run.log('processed_rows', row_count)\n",
                "\n",
                "# Save the prepped data\n",
                "print(\"Saving Data...\")\n",
                "os.makedirs(save_folder, exist_ok=True)\n",
                "save_path = os.path.join(save_folder,'data.csv')\n",
                "diabetes.to_csv(save_path, index=False, header=True)\n",
                "\n",
                "# End the run\n",
                "run.complete()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "y_bOoNkbuuOi"
            },
            "source": [
                "Vous pouvez maintenant cr\u00e9er le script pour la deuxi\u00e8me \u00e9tape, qui entra\u00eenera un mod\u00e8le. Le script inclut un argument nomm\u00e9 **--training-data**, qui fait r\u00e9f\u00e9rence \u00e0 l'emplacement o\u00f9 les donn\u00e9es pr\u00e9par\u00e9es ont \u00e9t\u00e9 enregistr\u00e9es \u00e0 l'\u00e9tape pr\u00e9c\u00e9dente.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "vscode": {
                    "languageId": "python"
                },
                "id": "iFlp5w2nuuOj"
            },
            "outputs": [],
            "source": [
                "%%writefile $experiment_folder/train_diabetes.py\n",
                "# Import libraries\n",
                "from azureml.core import Run, Model\n",
                "import argparse\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import joblib\n",
                "import os\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.metrics import roc_auc_score\n",
                "from sklearn.metrics import roc_curve\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Get parameters\n",
                "parser = argparse.ArgumentParser()\n",
                "parser.add_argument(\"--training-data\", type=str, dest='training_data', help='training data')\n",
                "args = parser.parse_args()\n",
                "training_data = args.training_data\n",
                "\n",
                "# Get the experiment run context\n",
                "run = Run.get_context()\n",
                "\n",
                "# load the prepared data file in the training folder\n",
                "print(\"Loading Data...\")\n",
                "file_path = os.path.join(training_data,'data.csv')\n",
                "diabetes = pd.read_csv(file_path)\n",
                "\n",
                "# Separate features and labels\n",
                "X, y = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, diabetes['Diabetic'].values\n",
                "\n",
                "# Split data into training set and test set\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
                "\n",
                "# Train adecision tree model\n",
                "print('Training a decision tree model...')\n",
                "model = DecisionTreeClassifier().fit(X_train, y_train)\n",
                "\n",
                "# calculate accuracy\n",
                "y_hat = model.predict(X_test)\n",
                "acc = np.average(y_hat == y_test)\n",
                "print('Accuracy:', acc)\n",
                "run.log('Accuracy', np.float(acc))\n",
                "\n",
                "# calculate AUC\n",
                "y_scores = model.predict_proba(X_test)\n",
                "auc = roc_auc_score(y_test,y_scores[:,1])\n",
                "print('AUC: ' + str(auc))\n",
                "run.log('AUC', np.float(auc))\n",
                "\n",
                "# plot ROC curve\n",
                "fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
                "fig = plt.figure(figsize=(6, 4))\n",
                "# Plot the diagonal 50% line\n",
                "plt.plot([0, 1], [0, 1], 'k--')\n",
                "# Plot the FPR and TPR achieved by our model\n",
                "plt.plot(fpr, tpr)\n",
                "plt.xlabel('False Positive Rate')\n",
                "plt.ylabel('True Positive Rate')\n",
                "plt.title('ROC Curve')\n",
                "run.log_image(name = \"ROC\", plot = fig)\n",
                "plt.show()\n",
                "\n",
                "# Save the trained model in the outputs folder\n",
                "print(\"Saving model...\")\n",
                "os.makedirs('outputs', exist_ok=True)\n",
                "model_file = os.path.join('outputs', 'diabetes_model.pkl')\n",
                "joblib.dump(value=model, filename=model_file)\n",
                "\n",
                "# Register the model\n",
                "print('Registering model...')\n",
                "Model.register(workspace=run.experiment.workspace,\n",
                "               model_path = model_file,\n",
                "               model_name = 'diabetes_model',\n",
                "               tags={'Training context':'Pipeline'},\n",
                "               properties={'AUC': np.float(auc), 'Accuracy': np.float(acc)})\n",
                "\n",
                "\n",
                "run.complete()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "VMVDO0ijuuOk"
            },
            "source": [
                "## Pr\u00e9parer un environnement de calcul pour les \u00e9tapes du pipeline\n",
                "\n",
                "Dans cet exercice, vous utiliserez le m\u00eame calcul pour les deux \u00e9tapes, mais il est important de r\u00e9aliser que chaque \u00e9tape est ex\u00e9cut\u00e9e ind\u00e9pendamment ; vous pouvez donc sp\u00e9cifier diff\u00e9rents contextes de calcul pour chaque \u00e9tape, le cas \u00e9ch\u00e9ant.\n",
                "\n",
                "Commencez par obtenir la cible de calcul que vous avez cr\u00e9\u00e9e dans un atelier pr\u00e9c\u00e9dent (si elle n'existe pas, elle sera cr\u00e9\u00e9e).\n",
                "\n",
                "> **Important**\u00a0: Remplacez *your-compute-cluster* par le nom de votre cluster de calcul dans le code ci-dessous avant de l'ex\u00e9cuter\u00a0! Les noms de cluster doivent \u00eatre des noms globalement uniques de 2 \u00e0 16 caract\u00e8res. Les caract\u00e8res valides sont les lettres, les chiffres et le caract\u00e8re -.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "gather": {
                    "logged": 1649367181165
                },
                "vscode": {
                    "languageId": "python"
                },
                "id": "8n02OwdbuuOl"
            },
            "outputs": [],
            "source": [
                "from azureml.core.compute import ComputeTarget, AmlCompute\n",
                "from azureml.core.compute_target import ComputeTargetException\n",
                "\n",
                "cluster_name = \"your-compute-cluster\"\n",
                "\n",
                "try:\n",
                "    # Check for existing compute target\n",
                "    pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
                "    print('Found existing cluster, use it.')\n",
                "except ComputeTargetException:\n",
                "    # If it doesn't already exist, create it\n",
                "    try:\n",
                "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
                "        pipeline_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
                "        pipeline_cluster.wait_for_completion(show_output=True)\n",
                "    except Exception as ex:\n",
                "        print(ex)\n",
                "    "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "9rJn-8KDuuOl"
            },
            "source": [
                "> **Remarque**\u00a0: Les instances de calcul et les clusters sont bas\u00e9s sur des images de machines virtuelles Azure standard. Pour cet exercice, l'image *Standard_DS11_v2* est recommand\u00e9e pour atteindre l'\u00e9quilibre optimal entre co\u00fbt et performances. Si votre abonnement a un quota qui n'inclut pas cette image, choisissez une autre image ; mais gardez \u00e0 l'esprit qu'une image plus grande peut entra\u00eener un co\u00fbt plus \u00e9lev\u00e9 et qu'une image plus petite peut ne pas \u00eatre suffisante pour accomplir les t\u00e2ches. Vous pouvez \u00e9galement demander \u00e0 votre administrateur Azure d'\u00e9tendre votre quota.\n",
                "\n",
                "Le calcul n\u00e9cessitera un environnement Python avec les d\u00e9pendances de package n\u00e9cessaires install\u00e9es.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "vscode": {
                    "languageId": "python"
                },
                "id": "Gc0Mgyu-uuOl"
            },
            "outputs": [],
            "source": [
                "%%writefile $experiment_folder/experiment_env.yml\n",
                "name: experiment_env\n",
                "dependencies:\n",
                "- python=3.6.2\n",
                "- scikit-learn\n",
                "- ipykernel\n",
                "- matplotlib\n",
                "- pandas\n",
                "- pip\n",
                "- pip:\n",
                "  - azureml-defaults\n",
                "  - pyarrow"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "xh74w9UQuuOm"
            },
            "source": [
                "Maintenant que vous disposez d'un fichier de configuration Conda, vous pouvez cr\u00e9er un environnement et l'utiliser dans la configuration d'ex\u00e9cution du pipeline.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "gather": {
                    "logged": 1649367186836
                },
                "vscode": {
                    "languageId": "python"
                },
                "id": "e2s4kawNuuOm"
            },
            "outputs": [],
            "source": [
                "from azureml.core import Environment\n",
                "from azureml.core.runconfig import RunConfiguration\n",
                "\n",
                "# Create a Python environment for the experiment (from a .yml file)\n",
                "experiment_env = Environment.from_conda_specification(\"experiment_env\", experiment_folder + \"/experiment_env.yml\")\n",
                "\n",
                "# Register the environment \n",
                "experiment_env.register(workspace=ws)\n",
                "registered_env = Environment.get(ws, 'experiment_env')\n",
                "\n",
                "# Create a new runconfig object for the pipeline\n",
                "pipeline_run_config = RunConfiguration()\n",
                "\n",
                "# Use the compute you created above. \n",
                "pipeline_run_config.target = pipeline_cluster\n",
                "\n",
                "# Assign the environment to the run configuration\n",
                "pipeline_run_config.environment = registered_env\n",
                "\n",
                "print (\"Run configuration created.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "9yMMn2DmuuOm"
            },
            "source": [
                "## Cr\u00e9er et ex\u00e9cuter un pipeline\n",
                "\n",
                "Vous \u00eates maintenant pr\u00eat \u00e0 cr\u00e9er et \u00e0 ex\u00e9cuter un pipeline.\n",
                "\n",
                "Vous devez d'abord d\u00e9finir les \u00e9tapes du pipeline et toutes les r\u00e9f\u00e9rences de donn\u00e9es qui doivent \u00eatre transmises entre elles. Dans ce cas, la premi\u00e8re \u00e9tape doit \u00e9crire les donn\u00e9es pr\u00e9par\u00e9es dans un dossier qui peut \u00eatre lu par la deuxi\u00e8me \u00e9tape. \u00c9tant donn\u00e9 que les \u00e9tapes seront ex\u00e9cut\u00e9es sur un calcul distant (et en fait, chacune pourrait \u00eatre ex\u00e9cut\u00e9e sur un calcul diff\u00e9rent), le chemin du dossier doit \u00eatre transmis en tant que r\u00e9f\u00e9rence de donn\u00e9es \u00e0 un emplacement dans un magasin de donn\u00e9es dans l'espace de travail. L'objet **OutputFileDatasetConfig** est un type sp\u00e9cial de r\u00e9f\u00e9rence de donn\u00e9es qui est utilis\u00e9 pour les emplacements de stockage provisoires qui peuvent \u00eatre transmis entre les \u00e9tapes du pipeline. Vous allez donc en cr\u00e9er un et l'utiliser comme sortie pour la premi\u00e8re \u00e9tape et comme entr\u00e9e pour le deuxi\u00e8me \u00e9tape. Notez que vous devez le transmettre en tant qu'argument de script afin que votre code puisse acc\u00e9der \u00e0 l'emplacement du magasin de donn\u00e9es r\u00e9f\u00e9renc\u00e9 par la r\u00e9f\u00e9rence de donn\u00e9es.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "gather": {
                    "logged": 1649367189787
                },
                "vscode": {
                    "languageId": "python"
                },
                "id": "GQLHuR_yuuOm"
            },
            "outputs": [],
            "source": [
                "from azureml.data import OutputFileDatasetConfig\n",
                "from azureml.pipeline.steps import PythonScriptStep\n",
                "\n",
                "# Get the training dataset\n",
                "diabetes_ds = ws.datasets.get(\"diabetes dataset\")\n",
                "\n",
                "# Create an OutputFileDatasetConfig (temporary Data Reference) for data passed from step 1 to step 2\n",
                "prepped_data = OutputFileDatasetConfig(\"prepped_data\")\n",
                "\n",
                "# Step 1, Run the data prep script\n",
                "prep_step = PythonScriptStep(name = \"Prepare Data\",\n",
                "                                source_directory = experiment_folder,\n",
                "                                script_name = \"prep_diabetes.py\",\n",
                "                                arguments = ['--input-data', diabetes_ds.as_named_input('raw_data'),\n",
                "                                             '--prepped-data', prepped_data],\n",
                "                                compute_target = pipeline_cluster,\n",
                "                                runconfig = pipeline_run_config,\n",
                "                                allow_reuse = True)\n",
                "\n",
                "# Step 2, run the training script\n",
                "train_step = PythonScriptStep(name = \"Train and Register Model\",\n",
                "                                source_directory = experiment_folder,\n",
                "                                script_name = \"train_diabetes.py\",\n",
                "                                arguments = ['--training-data', prepped_data.as_input()],\n",
                "                                compute_target = pipeline_cluster,\n",
                "                                runconfig = pipeline_run_config,\n",
                "                                allow_reuse = True)\n",
                "\n",
                "print(\"Pipeline steps defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "1evyjgcIuuOn"
            },
            "source": [
                "OK, vous \u00eates pr\u00eat \u00e0 cr\u00e9er le pipeline \u00e0 partir des \u00e9tapes que vous avez d\u00e9finies et \u00e0 l'ex\u00e9cuter en tant qu'exp\u00e9rience.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "gather": {
                    "logged": 1649367288774
                },
                "scrolled": false,
                "vscode": {
                    "languageId": "python"
                },
                "id": "ZWarRvXHuuOn"
            },
            "outputs": [],
            "source": [
                "from azureml.core import Experiment\n",
                "from azureml.pipeline.core import Pipeline\n",
                "from azureml.widgets import RunDetails\n",
                "\n",
                "# Construct the pipeline\n",
                "pipeline_steps = [prep_step, train_step]\n",
                "pipeline = Pipeline(workspace=ws, steps=pipeline_steps)\n",
                "print(\"Pipeline is built.\")\n",
                "\n",
                "# Create an experiment and run the pipeline\n",
                "experiment = Experiment(workspace=ws, name = 'mslearn-diabetes-pipeline')\n",
                "pipeline_run = experiment.submit(pipeline, regenerate_outputs=True)\n",
                "print(\"Pipeline submitted for execution.\")\n",
                "RunDetails(pipeline_run).show()\n",
                "pipeline_run.wait_for_completion(show_output=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "c4YTsyJMuuOn"
            },
            "source": [
                "Une repr\u00e9sentation graphique de l'exp\u00e9rience de pipeline s'affichera dans le widget lors de son ex\u00e9cution. Gardez un \u0153il sur l'indicateur du noyau en haut \u00e0 droite de la page, lorsqu'il passe de **\u26ab** \u00e0 **\u25ef**, le code a fini de s'ex\u00e9cuter. Vous pouvez \u00e9galement surveiller les ex\u00e9cutions de pipeline sur la page **Experiments** dans [Azure Machine Learning studio](https://ml.azure.com).\n",
                "\n",
                "Lorsque le pipeline est termin\u00e9, vous pouvez examiner les m\u00e9triques enregistr\u00e9es par ses ex\u00e9cutions enfants.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "gather": {
                    "logged": 1649367294378
                },
                "vscode": {
                    "languageId": "python"
                },
                "id": "47NiXPQhuuOn"
            },
            "outputs": [],
            "source": [
                "for run in pipeline_run.get_children():\n",
                "    print(run.name, ':')\n",
                "    metrics = run.get_metrics()\n",
                "    for metric_name in metrics:\n",
                "        print('\\t',metric_name, \":\", metrics[metric_name])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "zzXgApsJuuOo"
            },
            "source": [
                "En supposant que le pipeline a r\u00e9ussi, un nouveau mod\u00e8le doit \u00eatre enregistr\u00e9 avec une balise *Training context* indiquant qu'il a \u00e9t\u00e9 form\u00e9 dans un pipeline. Ex\u00e9cutez le code suivant pour v\u00e9rifier cela.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "gather": {
                    "logged": 1649367297400
                },
                "vscode": {
                    "languageId": "python"
                },
                "id": "UI4QEcKfuuOo"
            },
            "outputs": [],
            "source": [
                "from azureml.core import Model\n",
                "\n",
                "for model in Model.list(ws):\n",
                "    print(model.name, 'version:', model.version)\n",
                "    for tag_name in model.tags:\n",
                "        tag = model.tags[tag_name]\n",
                "        print ('\\t',tag_name, ':', tag)\n",
                "    for prop_name in model.properties:\n",
                "        prop = model.properties[prop_name]\n",
                "        print ('\\t',prop_name, ':', prop)\n",
                "    print('\\n')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "IDoTrEIsuuOo"
            },
            "source": [
                "## Publier le pipeline\n",
                "\n",
                "Apr\u00e8s avoir cr\u00e9\u00e9 et test\u00e9 un pipeline, vous pouvez le publier en tant que service REST.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "gather": {
                    "logged": 1649367300325
                },
                "vscode": {
                    "languageId": "python"
                },
                "id": "FkM2YWh-uuOo"
            },
            "outputs": [],
            "source": [
                "# Publish the pipeline from the run\n",
                "published_pipeline = pipeline_run.publish_pipeline(\n",
                "    name=\"diabetes-training-pipeline\", description=\"Trains diabetes model\", version=\"1.0\")\n",
                "\n",
                "published_pipeline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "gJkwFCWAuuOo"
            },
            "source": [
                "Notez que le pipeline publi\u00e9 a un point de terminaison, que vous pouvez voir sur la page **Points de terminaison** (sur l'onglet **Points de terminaison du pipeline**) dans [Azure Machine Learning studio](https://ml.azure.com) . Vous pouvez \u00e9galement trouver son URI en tant que propri\u00e9t\u00e9 de l'objet de pipeline publi\u00e9\u00a0:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "gather": {
                    "logged": 1649367303243
                },
                "vscode": {
                    "languageId": "python"
                },
                "id": "P00xONZ2uuOo"
            },
            "outputs": [],
            "source": [
                "rest_endpoint = published_pipeline.endpoint\n",
                "print(rest_endpoint)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "jWJAcifguuOo"
            },
            "source": [
                "## Appeler le point de terminaison du pipeline\n",
                "\n",
                "Pour utiliser le point de terminaison, les applications clientes doivent effectuer un appel REST via HTTP. Cette demande doit \u00eatre authentifi\u00e9e, donc un en-t\u00eate d'autorisation est requis. Une application r\u00e9elle n\u00e9cessiterait un principal de service avec lequel s'authentifier, mais pour le tester, nous utiliserons l'en-t\u00eate d'autorisation de votre connexion actuelle \u00e0 votre espace de travail Azure, que vous pouvez obtenir \u00e0 l'aide du code suivant\u00a0:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "gather": {
                    "logged": 1649367306323
                },
                "vscode": {
                    "languageId": "python"
                },
                "id": "Zmf3FdhPuuOo"
            },
            "outputs": [],
            "source": [
                "from azureml.core.authentication import InteractiveLoginAuthentication\n",
                "\n",
                "interactive_auth = InteractiveLoginAuthentication()\n",
                "auth_header = interactive_auth.get_authentication_header()\n",
                "print(\"Authentication header ready.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ldJwyYqHuuOp"
            },
            "source": [
                "Nous sommes maintenant pr\u00eats \u00e0 appeler l'interface REST. Le pipeline s'ex\u00e9cute de mani\u00e8re asynchrone, nous r\u00e9cup\u00e9rons donc un identifiant, que nous pouvons utiliser pour suivre l'exp\u00e9rience du pipeline pendant son ex\u00e9cution\u00a0:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "gather": {
                    "logged": 1649367309225
                },
                "vscode": {
                    "languageId": "python"
                },
                "id": "2RzbDLCfuuOp"
            },
            "outputs": [],
            "source": [
                "import requests\n",
                "\n",
                "experiment_name = 'mslearn-diabetes-pipeline'\n",
                "\n",
                "rest_endpoint = published_pipeline.endpoint\n",
                "response = requests.post(rest_endpoint, \n",
                "                         headers=auth_header, \n",
                "                         json={\"ExperimentName\": experiment_name})\n",
                "run_id = response.json()[\"Id\"]\n",
                "run_id"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "b2OWgtnjuuOp"
            },
            "source": [
                "Puisque vous avez l'ID d'ex\u00e9cution, vous pouvez l'utiliser pour attendre la fin de l'ex\u00e9cution.\n",
                "\n",
                "> **Remarque**\u00a0: Le pipeline devrait se terminer rapidement, car chaque \u00e9tape a \u00e9t\u00e9 configur\u00e9e pour permettre la r\u00e9utilisation de la sortie. Cela a \u00e9t\u00e9 fait principalement pour des raisons de commodit\u00e9 et pour gagner du temps dans ce cours. En r\u00e9alit\u00e9, vous voudriez probablement que la premi\u00e8re \u00e9tape s'ex\u00e9cute \u00e0 chaque fois au cas o\u00f9 les donn\u00e9es auraient chang\u00e9, et d\u00e9clencher les \u00e9tapes suivantes uniquement si la sortie de la premi\u00e8re \u00e9tape change.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "gather": {
                    "logged": 1649367316814
                },
                "vscode": {
                    "languageId": "python"
                },
                "id": "0FBJM00guuOp"
            },
            "outputs": [],
            "source": [
                "from azureml.pipeline.core.run import PipelineRun\n",
                "\n",
                "published_pipeline_run = PipelineRun(ws.experiments[experiment_name], run_id)\n",
                "published_pipeline_run.wait_for_completion(show_output=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "xS9OrDbNuuOp"
            },
            "source": [
                "## Planifier le pipeline\n",
                "\n",
                "Supposons que la clinique pour les patients diab\u00e9tiques collecte de nouvelles donn\u00e9es chaque semaine et les ajoute \u00e0 l'ensemble de donn\u00e9es. Vous pouvez ex\u00e9cuter le pipeline chaque semaine pour recycler le mod\u00e8le avec les nouvelles donn\u00e9es.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "gather": {
                    "logged": 1649367324073
                },
                "vscode": {
                    "languageId": "python"
                },
                "id": "UAnmC4vhuuOp"
            },
            "outputs": [],
            "source": [
                "from azureml.pipeline.core import ScheduleRecurrence, Schedule\n",
                "\n",
                "# Submit the Pipeline every Monday at 00:00 UTC\n",
                "recurrence = ScheduleRecurrence(frequency=\"Week\", interval=1, week_days=[\"Monday\"], time_of_day=\"00:00\")\n",
                "weekly_schedule = Schedule.create(ws, name=\"weekly-diabetes-training\", \n",
                "                                  description=\"Based on time\",\n",
                "                                  pipeline_id=published_pipeline.id, \n",
                "                                  experiment_name='mslearn-diabetes-pipeline', \n",
                "                                  recurrence=recurrence)\n",
                "print('Pipeline scheduled.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "DfdunyTFuuOp"
            },
            "source": [
                "Vous pouvez r\u00e9cup\u00e9rer les plannings d\u00e9finis dans l'espace de travail comme ceci\u00a0:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "gather": {
                    "logged": 1649367327077
                },
                "vscode": {
                    "languageId": "python"
                },
                "id": "4i2MYgxruuOp"
            },
            "outputs": [],
            "source": [
                "schedules = Schedule.list(ws)\n",
                "schedules"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "QBJP0A-IuuOp"
            },
            "source": [
                "Vous pouvez v\u00e9rifier la derni\u00e8re ex\u00e9cution comme ceci\u00a0:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "gather": {
                    "logged": 1649367330036
                },
                "vscode": {
                    "languageId": "python"
                },
                "id": "-Kg4LMeZuuOq"
            },
            "outputs": [],
            "source": [
                "pipeline_experiment = ws.experiments.get('mslearn-diabetes-pipeline')\n",
                "latest_run = list(pipeline_experiment.get_runs())[0]\n",
                "\n",
                "latest_run.get_details()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "UCXdyVHmuuOq"
            },
            "source": [
                "Ceci est un exemple simple, con\u00e7u pour d\u00e9montrer le principe. En r\u00e9alit\u00e9, vous pouvez cr\u00e9er une logique plus sophistiqu\u00e9e dans les \u00e9tapes du pipeline - par exemple, \u00e9valuer le mod\u00e8le par rapport \u00e0 certaines donn\u00e9es de test pour calculer une m\u00e9trique de performance comme l'AUC ou la pr\u00e9cision, comparer la m\u00e9trique \u00e0 celle de toutes les versions pr\u00e9c\u00e9demment enregistr\u00e9es du mod\u00e8le, et seulement enregistrer le nouveau mod\u00e8le s'il fonctionne mieux.\n",
                "\n",
                "Vous pouvez utiliser [l'extension Azure Machine Learning pour Azure DevOps](https://marketplace.visualstudio.com/items?itemName=ms-air-aiagility.vss-services-azureml) pour combiner les pipelines Azure ML avec les pipelines Azure DevOps ( oui, c'est * d\u00e9routant * qu'ils portent le m\u00eame nom\u00a0!) et int\u00e9grez le recyclage du mod\u00e8le dans un processus d' * int\u00e9gration continue/d\u00e9ploiement continu (CI/CD) *. Par exemple, vous pouvez utiliser un pipeline Azure DevOps *build* pour d\u00e9clencher un pipeline Azure ML qui entra\u00eene et enregistre un mod\u00e8le, et lorsque le mod\u00e8le est enregistr\u00e9, il peut d\u00e9clencher un pipeline Azure Devops *release* qui d\u00e9ploie le mod\u00e8le en tant que service Web, ainsi que l'application ou le service qui utilise le mod\u00e8le.\n"
            ]
        }
    ],
    "metadata": {
        "kernel_info": {
            "name": "python38-azureml"
        },
        "kernelspec": {
            "display_name": "Python 3.8 - AzureML",
            "language": "python",
            "name": "python38-azureml"
        },
        "nteract": {
            "version": "nteract-front-end@1.0.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}